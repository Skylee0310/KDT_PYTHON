{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "---\n",
    "- 둘 중 하나를 결정하는 문제 = 이진 분류(Binary Classification)\n",
    "- Regression이지만 분류인 이유\n",
    "    * => 선형 가설(y=wx+b)에 기반하여 분류하기때문에 Regression이라함\n",
    "- 이진 분류(Binary Classification)\n",
    "    * 0과 1로 결과가 나오는 분류\n",
    "    * 기존 LinearRegression의 선형함수만으로는 해당 S곡선 불가\n",
    "    * 0과 1 두 가지로 분류해주는 특별한 함수 필요 ==> 활성화함수(Activation Function)\n",
    "        - $ y=Wx+b $ ===> $ y=f(Wx+b)\n",
    "        - Sigmoid Function  \n",
    "        \n",
    "    * 비용 함수(Cost Function)\n",
    "        - 최적의 모델 파라미터를 찾을 수 있는 비용 함수(cost function) 정의\n",
    "        - torch.nn.functional의 binary_cross_entropy(예측값, 실제값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2568c4c69b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤 시드 설정 \n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [2., 3.],\n",
       "         [3., 1.],\n",
       "         [4., 3.],\n",
       "         [5., 3.],\n",
       "         [6., 2.]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]] # 6X2\n",
    "y_data = [[0], [0], [0], [1], [1], [1]] # 6X1\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# Sequential : 순서대로 진행되어야 하는 모듈을 묶음으로 관리.\n",
    "# (순서를 갖고 있는 모듈 컨테이너. 정의된 순서로 모든 모듈들을 통해 데이터 전달)\n",
    "# (여러 개의 레이어를 순차적으로 쌓아서 모델을 만들 수 있도록 한다.)\n",
    "\n",
    "model = nn.Sequential(\n",
    "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1 # w를 얘가 갖고 있음. 직접 만드는 것 X \n",
    "   nn.Sigmoid()     # 출력은 시그모이드 함수 거침 (★ 순서 중요! Linear 하는 결과가 Sigmoid에 들어가야함. 이진 분류라서 사용하는 것!)\n",
    ")\n",
    "\n",
    "# optimizer 설정\n",
    "# 확률적 경사 하강법으로 업데이트 해야하는 w,b는 모델 인스턴스에서 가지고 있음.\n",
    "# => 가지고 오는 메서드 = model.parameters()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "\n",
    "def training():\n",
    "    nb_epochs = 1000 # 1000번 학습\n",
    "    for epoch in range(nb_epochs + 1):\n",
    "        # 예측값 H(x) 계산\n",
    "        hypothesis = model(x_train) #훈련 데이터를 사용하여 모델의 예측값을 계산\n",
    "\n",
    "        # cost 계산 => 이진 분류 손실함수  binary_cross_entropy()\n",
    "        # y=0일 때 오차, y=1일 때 오차를 각각 계산.\n",
    "        cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "        # cost로 W,b값 업데이트 => SGD 경사하강법 방식으로 업데이트 진행 ---- 사실 경사하강법 이해 아직 못했음.\n",
    "        optimizer.zero_grad() # 초기화\n",
    "        cost.backward() # 역전파 : 손실 함수를 미분하여 모델의 파라미터에 대한 기울기(그래디언트)를 계산\n",
    "        optimizer.step() # 옵티마이저에 지정된 학습률(lr=0.1)에 따라 모델의 파라미터를 업데이트\n",
    "\n",
    "        # 10회마다 로그 출력\n",
    "        if epoch % 10 == 0:\n",
    "            prediction = hypothesis >= torch.FloatTensor([0.5])  # 예측값이 0.5를 넘으면 True로 간주\n",
    "            correct_prediction = prediction.float() == y_train # 정답 = 실제값(y_train)과 일치하는 경우만 True로 간주\n",
    "            accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도 = 정답 값의 합계 / 정답의 개수\n",
    "\n",
    "            print(f'Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f} Accuracy {accuracy * 100:2.2f}%')\n",
    "            # 현재 학습횟수 / 전체 학습 횟수   cost : 손실함수 값    Accuracy : 정확도(백분율로 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/1000 Cost: 0.614853 Accuracy 66.67%\n",
      "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.083805 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:\n",
      "tensor([-14.4839], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(model.parameters())) \n",
    "#모델 내의 모든 학습 가능한 파라미터가 리스트 형태로 출력\n",
    "#각 파라미터는 해당하는 레이어의 가중치(weight)와 편향(bias)로 구성\n",
    "# 이러한 파라미터들은 모델의 학습 과정에서 경사 하강법 등의 최적화 알고리즘을 사용하여 업데이트\n",
    "\n",
    "\n",
    "# 첫번째 파라미터 : tensor([[3.2477, 1.5151]], requires_grad=True) ==> 가중치\n",
    "# 두번째 파라미터 : tensor([-14.4576], requires_grad=True) ==> 편향(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
