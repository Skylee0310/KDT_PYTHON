{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN MODEL => Layer + AF(activation function)\n",
    "\n",
    "- layer 입력층 full connected Layer 즉 Linear\n",
    "- layer 은닉층\n",
    "- layer 은닉층\n",
    "\n",
    "...                ===> 2개 이상 (Deep Neuron Network)\n",
    "\n",
    "- layer 은닉층\n",
    "- layer 출력층\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "CNN MODEL => Layer + AF(활성화 함수) 이미지 처리 --- 수요일에 배울 내용\n",
    "\n",
    "- layer 입력층 convalution Layer + pooling layer\n",
    "- layer 은닉층 convalution Layer + pooling layer\n",
    "- layer 은닉층 convalution Layer + pooling layer\n",
    "- => 이미지 형태 그대로 사용.\n",
    "\n",
    "- layer 은닉층\n",
    "- layer 출력층\n",
    "- => 이미지 1D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "RNN MODEL\n",
    "\n",
    "- layer 입력층 Embedding Layer\n",
    "- layer 은닉층 convalution 1D layer\n",
    "- layer 은닉층 RNN/LSTM layer\n",
    "- => 텍스트 / 음성\n",
    "\n",
    "- layer 은닉층\n",
    "- layer 출력층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= 층만 바꾸면 된다는데...난 잘 모르겠고..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "overfitting\n",
    "- Train Dataset 늘리기 => 쉽지 않음. 비용, 인력.\n",
    "- feature의 개수 줄이기 => 저차원\n",
    "- regulaization 규제 term 추가\n",
    "- Dropout Layer 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Dropout Layer\n",
    "- 동작 : 일부 노드/퍼셉트론 무작위 비활성화\n",
    "- 효과 : Overfitting 방지\n",
    "\n",
    "매번 다른 형태 노드 학습 진행 >>> 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout 기능 --> 학습 시 과대적합 해결하려고 쓰는 건데 평가할 때는 안됨..!\n",
    "- 모델에 과대적합 나오면 이것부터 씀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS torch.nn.Dropout( p=0.5, inplace=False )\n",
    "\n",
    "- 훈련 중 확률 사용하여 입력 텐서의 일부 요소를 무작위로 0으로 설정\n",
    "- 연산 진행 되지 않음\n",
    "- 매개변수\n",
    "• p : Layer 내 노드/퍼셉트론이 0이 될 확률 [ 기본값: 0.5 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.dropout = nn.Dropout(0.3)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Vanishing & Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해결방법 : 가중치 초기화 (Weight initialization = 세비어(Xavier) / 글로럿(Glorot)) \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module) :\n",
    "\n",
    "    # 모델 구성 요소 생성 및 구조 설정\n",
    "    def __init__(self) : \n",
    "        super(Net, self).__init__() # super 안에 자식클래스 이름과 self는 생략 가능.\n",
    "\n",
    "        self.fc1 = nn.Linear(8,4) # fc1 = layer1\n",
    "        self.fc2 = nn.Linear(4,2)\n",
    "        self.fc3 = nn.Linear(2,1)\n",
    "    \n",
    "    # 순방향 학습 진행 함수\n",
    "    def forword(self, x) :\n",
    "        return self.fc3(self.fc2(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (fc3): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Net                                      --\n",
       "├─Linear: 1-1                            36\n",
       "├─Linear: 1-2                            10\n",
       "├─Linear: 1-3                            3\n",
       "=================================================================\n",
       "Total params: 49\n",
       "Trainable params: 49\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model=Net()\n",
    "\n",
    "print(model)\n",
    "\n",
    "summary(model)\n",
    "\n",
    "# w + b\n",
    "# 8 + 1\n",
    "# 8 + 1\n",
    "# 8 + 1\n",
    "# 8 + 1\n",
    "# ===> total 36\n",
    "\n",
    "# 4 + 1\n",
    "# 4 + 1\n",
    "# ===> total 10\n",
    "\n",
    "# 2 + 1\n",
    "# ===> total 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.2321,  0.0932, -0.3271,  0.2081,  0.1896, -0.1728, -0.1957, -0.1939],\n",
       "         [-0.1712, -0.0751, -0.3102,  0.3185, -0.0249, -0.2731, -0.1012,  0.0694],\n",
       "         [ 0.2828, -0.2533, -0.0794,  0.0841,  0.1798, -0.3171,  0.2283,  0.1826],\n",
       "         [-0.3300,  0.1637, -0.3419, -0.1940, -0.2468, -0.0697,  0.0056,  0.0754]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3278,  0.0250,  0.1161,  0.3358], requires_grad=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 인스턴스 속성\n",
    "# 모델의 특정 층 추출\n",
    "model.fc1.weight, model.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2321,  0.0932, -0.3271,  0.2081,  0.1896, -0.1728, -0.1957, -0.1939],\n",
      "        [-0.1712, -0.0751, -0.3102,  0.3185, -0.0249, -0.2731, -0.1012,  0.0694],\n",
      "        [ 0.2828, -0.2533, -0.0794,  0.0841,  0.1798, -0.3171,  0.2283,  0.1826],\n",
      "        [-0.3300,  0.1637, -0.3419, -0.1940, -0.2468, -0.0697,  0.0056,  0.0754]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.3278,  0.0250,  0.1161,  0.3358], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2659, -0.2018, -0.1910,  0.3859],\n",
      "        [ 0.2206,  0.1302,  0.1158, -0.0640]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.2435, 0.4871], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.6310, 0.4295]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.0871], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델의 각 층별 w, b 텐서 정보 확인\n",
    "for param in model.parameters() :\n",
    "    print(param, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.2321,  0.0932, -0.3271,  0.2081,  0.1896, -0.1728, -0.1957, -0.1939],\n",
      "        [-0.1712, -0.0751, -0.3102,  0.3185, -0.0249, -0.2731, -0.1012,  0.0694],\n",
      "        [ 0.2828, -0.2533, -0.0794,  0.0841,  0.1798, -0.3171,  0.2283,  0.1826],\n",
      "        [-0.3300,  0.1637, -0.3419, -0.1940, -0.2468, -0.0697,  0.0056,  0.0754]],\n",
      "       requires_grad=True))\n",
      "\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.3278,  0.0250,  0.1161,  0.3358], requires_grad=True))\n",
      "\n",
      "('fc2.weight', Parameter containing:\n",
      "tensor([[ 0.2659, -0.2018, -0.1910,  0.3859],\n",
      "        [ 0.2206,  0.1302,  0.1158, -0.0640]], requires_grad=True))\n",
      "\n",
      "('fc2.bias', Parameter containing:\n",
      "tensor([0.2435, 0.4871], requires_grad=True))\n",
      "\n",
      "('fc3.weight', Parameter containing:\n",
      "tensor([[0.6310, 0.4295]], requires_grad=True))\n",
      "\n",
      "('fc3.bias', Parameter containing:\n",
      "tensor([0.0871], requires_grad=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters() :\n",
    "    print(param, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=8, out_features=4, bias=True)\n",
      "\n",
      "Linear(in_features=4, out_features=2, bias=True)\n",
      "\n",
      "Linear(in_features=2, out_features=1, bias=True)\n",
      "\n",
      "('fc1', Linear(in_features=8, out_features=4, bias=True))\n",
      "\n",
      "('fc2', Linear(in_features=4, out_features=2, bias=True))\n",
      "\n",
      "('fc3', Linear(in_features=2, out_features=1, bias=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성하는 모든 layer 가져오기\n",
    "\n",
    "for child in model.children() :\n",
    "    print(child, end='\\n\\n')\n",
    "\n",
    "for child in model.named_children() :\n",
    "    print(child, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layer의 가중치 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4136,  0.1569,  0.1981,  0.2164,  0.2535,  0.3027, -0.2740,  0.2569],\n",
       "        [-0.4111,  0.4686,  0.2546, -0.2410,  0.4218, -0.5349, -0.0713,  0.1359],\n",
       "        [ 0.4340,  0.6735, -0.0613, -0.2331,  0.6571, -0.3274, -0.3752,  0.5281],\n",
       "        [ 0.2872, -0.5828, -0.0871, -0.1526, -0.4463, -0.1522,  0.4270,  0.1775]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세비어 알고리즘의 가중치 초기화\n",
    "nn.init.xavier_uniform_(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0419, -0.6099,  0.1906, -0.4992,  0.6398,  0.4509,  0.4221,  1.1307],\n",
       "        [ 0.2579, -0.2435,  0.7617, -0.5576, -0.6433,  0.8052,  0.6637, -0.4314],\n",
       "        [ 0.3286, -0.0411, -0.1039, -0.7153,  0.0186, -0.3355,  0.2184, -1.2126],\n",
       "        [ 0.4627,  0.0273,  1.3195, -0.5464,  0.5824, -0.2040, -0.4065, -0.1575]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤 알고리즘의 가중치 초기화\n",
    "nn.init.kaiming_normal_(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 layer에 대하여 xavier uniform initialization 설정\n",
    "for name, child in model.named_children() :\n",
    "    nn.init.xavier_uniform_(child.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
