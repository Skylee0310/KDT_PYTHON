{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미지 데이터 로딩\n",
    "file = '../data/image/xuguanghan.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('JPEG', 574, 550, 'RGB', PIL.JpegImagePlugin.JpegImageFile)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catImg = Image.open(fp=file)\n",
    "catImg.format, catImg.height, catImg.width, catImg.mode, type(catImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 574, 550]) 3\n",
      "tensor([[0.6667, 0.6824, 0.6980,  ..., 0.4039, 0.4392, 0.4667],\n",
      "        [0.6745, 0.6863, 0.6902,  ..., 0.3961, 0.4235, 0.4431],\n",
      "        [0.6745, 0.6824, 0.6902,  ..., 0.4118, 0.4431, 0.4627],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 사용법 \n",
    "# (1) 인스턴스 생성 \n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# (2) 인스턴스변수 사용\n",
    "catTS = to_tensor(catImg)\n",
    "\n",
    "# (3) 변환된 이미지 텐서 확인\n",
    "print(catTS.shape, catTS.ndim)\n",
    "print(catTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (574, 550, 3) 3\n",
      "<class 'numpy.ndarray'> (574, 550, 3) 3\n",
      "torch.Size([3, 574, 550]) 3\n",
      "tensor([[0.7647, 0.7804, 0.7961,  ..., 0.4510, 0.4863, 0.5137],\n",
      "        [0.7725, 0.7843, 0.7882,  ..., 0.4431, 0.4706, 0.4902],\n",
      "        [0.7725, 0.7804, 0.7882,  ..., 0.4549, 0.4863, 0.5059],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000]])\n",
      "\n",
      "torch.Size([3, 574, 550]) 3\n",
      "tensor([[0.6667, 0.6824, 0.6980,  ..., 0.4039, 0.4392, 0.4667],\n",
      "        [0.6745, 0.6863, 0.6902,  ..., 0.3961, 0.4235, 0.4431],\n",
      "        [0.6745, 0.6824, 0.6902,  ..., 0.4157, 0.4471, 0.4667],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "## open cv\n",
    "import cv2\n",
    "img = cv2.imread(filename=file) # BGR\n",
    "img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # RGB\n",
    "\n",
    "print(type(img), img.shape, img.ndim)\n",
    "print(type(img2), img2.shape, img2.ndim)\n",
    "\n",
    "## 텐서화\n",
    "cat2 = to_tensor(img)\n",
    "cat3 = to_tensor(img2)\n",
    "\n",
    "print(cat2.shape, cat2.ndim)\n",
    "print(cat2[0])\n",
    "print()\n",
    "print(cat3.shape, cat3.ndim)\n",
    "print(cat3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (574, 550, 3) 3\n"
     ]
    }
   ],
   "source": [
    "# Resizing\n",
    "resize = transforms.Resize()\n",
    "resize((50,50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = transforms.Compose(\n",
    "    [transforms.Resize(size=(50,50)), # 넘파이 안됨..\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # 원하는 만큼 돌려놓기.\n",
    "    transforms.ToTensor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7294, 0.7961, 0.8196,  ..., 0.5216, 0.5020, 0.4471],\n",
       "         [0.7294, 0.8000, 0.8196,  ..., 0.6118, 0.6196, 0.6235],\n",
       "         [0.7294, 0.7961, 0.8196,  ..., 0.6706, 0.6706, 0.6706],\n",
       "         ...,\n",
       "         [0.7333, 0.7882, 0.8000,  ..., 0.8588, 0.7608, 0.7843],\n",
       "         [0.7843, 0.8275, 0.8392,  ..., 0.9059, 0.8235, 0.8157],\n",
       "         [0.9725, 0.9804, 0.9804,  ..., 0.9882, 0.9804, 0.9804]],\n",
       "\n",
       "        [[0.7333, 0.8039, 0.8275,  ..., 0.5176, 0.4941, 0.4353],\n",
       "         [0.7333, 0.8039, 0.8275,  ..., 0.6118, 0.6196, 0.6196],\n",
       "         [0.7373, 0.8000, 0.8275,  ..., 0.6745, 0.6706, 0.6667],\n",
       "         ...,\n",
       "         [0.7412, 0.7922, 0.8078,  ..., 0.8588, 0.7686, 0.7843],\n",
       "         [0.7882, 0.8314, 0.8392,  ..., 0.9020, 0.8275, 0.8196],\n",
       "         [0.9765, 0.9804, 0.9804,  ..., 0.9882, 0.9804, 0.9804]],\n",
       "\n",
       "        [[0.8235, 0.8667, 0.8784,  ..., 0.5647, 0.5373, 0.4784],\n",
       "         [0.8235, 0.8706, 0.8824,  ..., 0.6667, 0.6824, 0.6824],\n",
       "         [0.8235, 0.8706, 0.8824,  ..., 0.7490, 0.7373, 0.7373],\n",
       "         ...,\n",
       "         [0.8275, 0.8667, 0.8784,  ..., 0.9255, 0.8549, 0.8745],\n",
       "         [0.8549, 0.8902, 0.8941,  ..., 0.9490, 0.8941, 0.8902],\n",
       "         [0.9804, 0.9843, 0.9843,  ..., 0.9922, 0.9843, 0.9843]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(catImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 데이터 셋 <hr>\n",
    "- torchvision.ImageFolder 클래스 사용\n",
    "   * 이미지 데이터 라벨링\n",
    "   * 이미지 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '../data/image/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDS = ImageFolder(root=img_root,\n",
    "            transform = preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['afolder'], {'afolder': 0}, [('../data/image/afolder\\\\gray_angry.jpg', 0)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgDS.classes, imgDS.class_to_idx, imgDS.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 50, 50]) torch.Size([1]) tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "imgDL=DataLoader(imgDS)\n",
    "\n",
    "for (img, label) in imgDL :\n",
    "    print(img.shape, label.shape, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 이미지 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
